{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kSeuVEbFxA4",
        "outputId": "69d6a779-7372-4cd4-ddaf-a5378b5ba747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contamination: 0.1 ; Gaps: 1 ; R_squared: 0.071 ; RMSE: 329.909\n",
            "Contamination: 0.1 ; Gaps 5 ; R_squared: 0.064 ; RMSE: 313.796\n",
            "Contamination: 0.1 ; Gaps 8 ; R_squared: 0.087 ; RMSE: 318.566\n",
            "Contamination: 0.1 ; Gaps 20 ; R_squared: 0.026 ; RMSE: 336.87\n",
            "Contamination: 0.1 ; Gaps 40 ; R_squared: 0.026 ; RMSE: 360.975\n",
            "Contamination: 0.2 ; Gaps 1 ; R_squared: 0.027 ; RMSE: 355.998\n",
            "Contamination: 0.2 ; Gaps 5 ; R_squared: 0.033 ; RMSE: 348.649\n",
            "Contamination: 0.2 ; Gaps: 8 ; R_squared: 0.096 ; RMSE: 307.038\n",
            "Contamination: 0.2 ; Gaps: 20 ; R_squared: 0.082 ; RMSE: 321.753\n",
            "Contamination: 0.2 ; Gaps 40 ; R_squared: 0.028 ; RMSE: 335.692\n",
            "Contamination: 0.3 ; Gaps 1 ; R_squared: 0.043 ; RMSE: 300.248\n",
            "Contamination: 0.3 ; Gaps 5 ; R_squared: 0.058 ; RMSE: 306.577\n",
            "Contamination: 0.3 ; Gaps 8 ; R_squared: 0.029 ; RMSE: 401.063\n",
            "Contamination: 0.3 ; Gaps: 20 ; R_squared: 0.082 ; RMSE: 315.491\n",
            "Contamination: 0.3 ; Gaps: 40 ; R_squared: 0.077 ; RMSE: 316.819\n",
            "Contamination: 0.4 ; Gaps 1 ; R_squared: 0.034 ; RMSE: 341.666\n",
            "Contamination: 0.4 ; Gaps 5 ; R_squared: 0.039 ; RMSE: 363.425\n",
            "Contamination: 0.4 ; Gaps 8 ; R_squared: 0.038 ; RMSE: 347.588\n",
            "Contamination: 0.4 ; Gaps: 20 ; R_squared: 0.079 ; RMSE: 315.396\n",
            "Contamination: 0.4 ; Gaps 40 ; R_squared: 0.03 ; RMSE: 347.421\n",
            "Contamination: 0.5 ; Gaps 1 ; R_squared: 0.047 ; RMSE: 322.107\n",
            "Contamination: 0.5 ; Gaps: 5 ; R_squared: 0.073 ; RMSE: 342.504\n",
            "Contamination: 0.5 ; Gaps: 8 ; R_squared: 0.087 ; RMSE: 325.993\n",
            "Contamination: 0.5 ; Gaps: 20 ; R_squared: 0.079 ; RMSE: 316.051\n",
            "Contamination: 0.5 ; Gaps 40 ; R_squared: 0.029 ; RMSE: 332.289\n",
            "Best: R_squared: 0.096 ; Contamination: 0.2 ; Gaps: 8 ; Lag: 0 ; RMSE: 307.038\n"
          ]
        }
      ],
      "source": [
        "# @title Storm Regression Model Selection\n",
        "import math\n",
        "def mapping_series(data_np,window_size,window_hop):\n",
        "\n",
        "  # Break series into a fixed window length\n",
        "  data_np = data_np.values\n",
        "  start_frame = window_size\n",
        "  end_frame = window_hop * math.floor(float(len(data_np)) / window_hop)\n",
        "  win_step = window_size - window_hop\n",
        "  wd = []\n",
        "  for frame_idx in range(start_frame, end_frame, win_step):\n",
        "      window = data_np[frame_idx-window_size:frame_idx]\n",
        "      wd.append(window)\n",
        "\n",
        "  # Convert list to array\n",
        "  w_props = np.array(wd)\n",
        "  # Slice and convert array to temp dataframe\n",
        "  temp_f = w_props[:,:,0]\n",
        "  preci_f = w_props[:,:,1]\n",
        "  gust_f = w_props[:,:,2]\n",
        "  wind_f = w_props[:,:,3]\n",
        "  windir_f = w_props[:,:,4]\n",
        "  lai_f = w_props[:,:,5]\n",
        "\n",
        "  # Build relationship with weather predictors\n",
        "  # Take min and max of temperature\n",
        "  t_min = DataFrame(temp_f)\n",
        "  t_min = t_min.min(axis=1)\n",
        "  t_max = DataFrame(temp_f)\n",
        "  t_max = t_max.max(axis=1)\n",
        "  # Take max and avg of precipitation\n",
        "  p_max = DataFrame(preci_f)\n",
        "  p_max = p_max.max(axis=1)\n",
        "  p_avg = DataFrame(preci_f)\n",
        "  p_avg = p_avg.mean(axis=1)\n",
        "  # Take max and avg of gust\n",
        "  g_max = DataFrame(gust_f)\n",
        "  g_max = g_max.max(axis=1)\n",
        "  g_avg = DataFrame(gust_f)\n",
        "  g_avg = g_avg.mean(axis=1)\n",
        "  # Take max and avg of wind\n",
        "  w_max = DataFrame(wind_f)\n",
        "  w_max = w_max.max(axis=1)\n",
        "  w_avg = DataFrame(wind_f)\n",
        "  w_avg = w_avg.mean(axis=1)\n",
        "  # Take avg and std or variance (var) of wind direction\n",
        "  wdir_std = DataFrame(windir_f)\n",
        "  wdir_std = wdir_std.std(axis=1)\n",
        "  wdir_avg = DataFrame(windir_f)\n",
        "  wdir_avg = wdir_avg.mean(axis=1)\n",
        "  # Take max or min of leaf area index\n",
        "  lai_avg = DataFrame(lai_f)\n",
        "  lai_avg = lai_avg.max(axis=1)\n",
        "\n",
        "  # Convert weather predictors to numpy array\n",
        "  t_min = np.array(t_min)\n",
        "  t_max = np.array(t_max)\n",
        "  p_max = np.array(p_max)\n",
        "  p_avg = np.array(p_avg)\n",
        "  g_max = np.array(g_max)\n",
        "  g_avg = np.array(g_avg)\n",
        "  w_max = np.array(w_max)\n",
        "  w_avg = np.array(w_avg)\n",
        "  wdir_avg = np.array(wdir_avg)\n",
        "  wdir_std = np.array(wdir_std)\n",
        "  lai_avg = np.array(lai_avg)\n",
        "\n",
        "  # Concatenate weather predictors\n",
        "  w_preds = np.concatenate([t_min[:, None],\n",
        "                                    t_max[:, None],\n",
        "                                    p_max[:, None],\n",
        "                                    p_avg[:, None],\n",
        "                                    g_max[:, None],\n",
        "                                    g_avg[:, None],\n",
        "                                    w_max[:, None],\n",
        "                                    w_avg[:, None],\n",
        "                                    wdir_avg[:, None],\n",
        "                                    wdir_std[:, None],\n",
        "                                    lai_avg[:, None],], axis=1)\n",
        "  preds = pd.DataFrame(w_preds,columns=['Temp_min','Temp_max','Preci_max','Preci_avg',\n",
        "                                        'Gust_max','Gust_avg','Wind_max','Wind_avg',\n",
        "                                        'Windir_avg','Windir_std','LAI_avg'])\n",
        "  return preds\n",
        "\n",
        "#@title compute_samples_x_y_zero_train\n",
        "def compute_samples_x_y_zero_train(vals_set,lag_ext,ff):\n",
        "  # retrieve train data intervals of windows\n",
        "  X_st_s = []\n",
        "  y_st_s = []\n",
        "  br = 1\n",
        "  X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "  if isinstance(ff[0], list):\n",
        "    fff = ff[0]\n",
        "    for c in range(fff[0],fff[1]+1):\n",
        "      df_w_cv = vals_set[c]\n",
        "      df_ws_cv = df_w_cv[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "      df_outage_cv = df_w_cv[['id','Total Outages']]\n",
        "\n",
        "      if df_ws_cv.isnull().values.any():\n",
        "        # Interpolate missing values\n",
        "        df_ws_cv = df_ws_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_ws_cv['id'] = df_ws_cv['id'].astype(int)\n",
        "        df_outage_cv = df_outage_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_outage_cv['id'] = df_outage_cv['id'].astype(int)\n",
        "\n",
        "      df_win_drop = df_ws_cv.drop('id', axis=1)\n",
        "      df_outage_drop_cv = df_outage_cv.drop('id', axis=1)\n",
        "\n",
        "      # add relative normalized time points\n",
        "      pt_array = np.arange(len(df_win_drop))\n",
        "      df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in pt_array]\n",
        "      df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "      df_win_no_lags = df_win_drop.reset_index(drop=True)\n",
        "      df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "      df_outage_drop_cv = df_outage_drop_cv.reset_index(drop=True)\n",
        "      df_outage_drop_vals_cv = df_outage_drop_cv.values\n",
        "\n",
        "      xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                      df_outage_drop_vals_cv,lag_ext+1)\n",
        "      X_st_s.append(xtrain_storms_no_lags)\n",
        "      y_st_s.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "    ffk = ff[1]\n",
        "    for c in range(ffk[0],ffk[1]+1):\n",
        "      df_w_cv = vals_set[c]\n",
        "      df_ws_cv = df_w_cv[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "      df_outage_cv = df_w_cv[['id','Total Outages']]\n",
        "\n",
        "      if df_ws_cv.isnull().values.any():\n",
        "        # Interpolate missing values\n",
        "        df_ws_cv = df_ws_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_ws_cv['id'] = df_ws_cv['id'].astype(int)\n",
        "        df_outage_cv = df_outage_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_outage_cv['id'] = df_outage_cv['id'].astype(int)\n",
        "\n",
        "      df_win_drop = df_ws_cv.drop('id', axis=1)\n",
        "      df_outage_drop_cv = df_outage_cv.drop('id', axis=1)\n",
        "\n",
        "      # add relative normalized time points\n",
        "      pt_array = np.arange(len(df_win_drop))\n",
        "      df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in pt_array]\n",
        "      df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "\n",
        "      df_win_no_lags = df_win_drop.reset_index(drop=True)\n",
        "      df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "      df_outage_drop_cv = df_outage_drop_cv.reset_index(drop=True)\n",
        "      df_outage_drop_cv_vals = df_outage_drop_cv.values\n",
        "\n",
        "      xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                      df_outage_drop_cv_vals,lag_ext+1)\n",
        "      X_st_s.append(xtrain_storms_no_lags)\n",
        "      y_st_s.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "    sample_X = np.vstack(X_st_s)\n",
        "    sample_y = pd.concat(y_st_s)\n",
        "    sample_X = np.array(sample_X)\n",
        "    xtr_reshape = sample_X.reshape(sample_X.shape[0],sample_X.shape[1]*sample_X.shape[-1])\n",
        "\n",
        "    # Normalize regression varaibles\n",
        "    xtr_scale =   X_scaler.fit_transform(xtr_reshape)\n",
        "    sample_y_scale = Y_scaler.fit_transform(sample_y)\n",
        "    return (xtr_scale,sample_y_scale,X_scaler,Y_scaler)\n",
        "\n",
        "  else:\n",
        "    X_st_s = []\n",
        "    y_st_s = []\n",
        "    br = 1\n",
        "    X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    for c in range(ff[0],ff[1]+1):\n",
        "      df_w_cv = vals_set[c]\n",
        "      df_ws_cv = df_w_cv[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "      df_outage_cv = df_w_cv[['id','Total Outages']]\n",
        "\n",
        "      if df_ws_cv.isnull().values.any():\n",
        "        # Interpolate missing values\n",
        "        df_ws_cv = df_ws_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_ws_cv['id'] = df_ws_cv['id'].astype(int)\n",
        "        df_outage_cv = df_outage_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_outage_cv['id'] = df_outage_cv['id'].astype(int)\n",
        "\n",
        "      df_win_drop = df_ws_cv.drop('id', axis=1)\n",
        "      df_outage_drop_cv = df_outage_cv.drop('id', axis=1)\n",
        "\n",
        "      # add relative normalized time points\n",
        "      pt_array = np.arange(len(df_win_drop))\n",
        "      df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in pt_array]\n",
        "      df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "      df_win_no_lags = df_win_drop.reset_index(drop=True)\n",
        "      df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "      df_outage_drop_cv = df_outage_drop_cv.reset_index(drop=True)\n",
        "      df_outage_drop_cv_vals = df_outage_drop_cv.values\n",
        "\n",
        "      xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                      df_outage_drop_cv_vals,lag_ext+1)\n",
        "      X_st_s.append(xtrain_storms_no_lags)\n",
        "      y_st_s.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "    sample_X = np.vstack(X_st_s)\n",
        "    sample_y = pd.concat(y_st_s)\n",
        "    sample_X = np.array(sample_X)\n",
        "    xtr_reshape = sample_X.reshape(sample_X.shape[0],sample_X.shape[1]*sample_X.shape[-1])\n",
        "\n",
        "    # Normalize regression variables\n",
        "    xtr_scale = X_scaler.fit_transform(xtr_reshape)\n",
        "    sample_y_scale = Y_scaler.fit_transform(sample_y)\n",
        "    return (xtr_scale,sample_y_scale,X_scaler,Y_scaler)\n",
        "\n",
        "#@title ml val iforest\n",
        "from sklearn.ensemble import IsolationForest\n",
        "def ifor_vals(val_set):\n",
        "    clf_val =IsolationForest(max_samples='auto',random_state=rng)\n",
        "    clf_val.fit(val_set)\n",
        "    wea_pd_val = pd.DataFrame(val_set)\n",
        "    new_val_set = wea_pd_val.copy()\n",
        "    new_val_set['scores'] = clf_val.decision_function(wea_pd_val)\n",
        "    new_val_set['anomaly'] = clf_val.predict(wea_pd_val)\n",
        "    new_val_set['anomaly'] = new_val_set['anomaly'].map({1:0,-1:1})\n",
        "    return new_val_set\n",
        "\n",
        "#@title compute_samples_x_y_zero_test\n",
        "import numpy as np\n",
        "def compute_samples_x_y_zero_test(vals_set,lag_ext,ff):\n",
        "  # retrieve train data intervals of windows\n",
        "  X_st_s = []\n",
        "  y_st_s = []\n",
        "\n",
        "  for c in range(ff[0],ff[1]+1):\n",
        "    df_w_cv_test = vals_set[c]\n",
        "    df_ws_cv_test = df_w_cv_test[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "    df_outage_cv_test = df_w_cv_test[['id','Total Outages']]\n",
        "\n",
        "    if df_ws_cv_test.isnull().values.any():\n",
        "        # Interpolate missing values\n",
        "        df_ws_cv_test = df_ws_cv_test.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_ws_cv_test['id'] = df_ws_cv_test['id'].astype(int)\n",
        "\n",
        "    df_win_drop = df_ws_cv_test.drop('id', axis=1)\n",
        "    df_outage_drop_cv_test = df_outage_cv_test.drop('id', axis=1)\n",
        "\n",
        "    # add relative normalized time points\n",
        "    pte_array = np.arange(len(df_win_drop))\n",
        "    df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in pte_array]\n",
        "    df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "    df_win_no_lags = df_win_drop.reset_index(drop=True)\n",
        "    df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "    df_outage_drop_cv_test = df_outage_drop_cv_test.reset_index(drop=True)\n",
        "    df_outage_drop_cv_test_vals = df_outage_drop_cv_test.values\n",
        "\n",
        "    xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                    df_outage_drop_cv_test_vals,lag_ext+1)\n",
        "    X_st_s.append(xtrain_storms_no_lags)\n",
        "    y_st_s.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "  sample_X = np.vstack(X_st_s)\n",
        "  sample_y = pd.concat(y_st_s)\n",
        "  sample_X = np.array(sample_X)\n",
        "  xtr_reshape = sample_X.reshape(sample_X.shape[0],sample_X.shape[1]*sample_X.shape[-1])\n",
        "\n",
        "  return (xtr_reshape,sample_y,y_st_s)\n",
        "\n",
        "\n",
        "#@title compute_samples_x_y_train_multiple\n",
        "import numpy as np\n",
        "def compute_samples_x_y_train_multiple(vals_set,ff,lag_ext,raw_trr):\n",
        "  X_st_s_full = []\n",
        "  y_st_s_full = []\n",
        "\n",
        "  X_scaler_full = MinMaxScaler(feature_range=(0, 1))\n",
        "  Y_scaler_full = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "  if isinstance(ff[0], list):\n",
        "    fff = ff[0]\n",
        "    for c in range(fff[0],fff[1]+1):\n",
        "      df_w_cv = vals_set[c]\n",
        "      df_ws_cv = df_w_cv[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "      df_outage_cv = df_w_cv[['id','Total Outages']]\n",
        "      df_outage_drop_cv = df_outage_cv.drop('id', axis=1)\n",
        "\n",
        "      if df_ws_cv.isnull().values.any():\n",
        "        # Interpolate missing values\n",
        "        df_ws_cv = df_ws_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_ws_cv['id'] = df_ws_cv['id'].astype(int)\n",
        "\n",
        "      df_lags = get_lags(df_ws_cv,lag_ext,raw_trr)\n",
        "      df_win_drop = df_lags.drop('Total Outages', axis=1)\n",
        "\n",
        "      # add relative normalized time points\n",
        "      ptef_array = np.arange(len(df_win_drop))\n",
        "      df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in ptef_array]\n",
        "      df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "      df_win_no_lags = df_win_drop.reset_index(drop=True)\n",
        "      df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "      df_outage_drop_cv = df_outage_drop_cv.reset_index(drop=True)\n",
        "      df_outage_drop_cv_vals = df_outage_drop_cv.values\n",
        "\n",
        "      xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                      df_outage_drop_cv_vals,lag_ext+1)\n",
        "      X_st_s_full.append(xtrain_storms_no_lags)\n",
        "      y_st_s_full.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "    ffk = ff[1]\n",
        "    for d in range(ffk[0],ffk[1]+1):\n",
        "      df_w_cv = vals_set[d]\n",
        "      df_ws_cv = df_w_cv[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "      df_outage_cv = df_w_cv[['id','Total Outages']]\n",
        "      df_outage_drop_cv = df_outage_cv.drop('id', axis=1)\n",
        "\n",
        "      if df_ws_cv.isnull().values.any():\n",
        "        # Interpolate missing values\n",
        "        df_ws_cv = df_ws_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_ws_cv['id'] = df_ws_cv['id'].astype(int)\n",
        "\n",
        "      df_lags = get_lags(df_ws_cv,lag_ext,raw_trr)\n",
        "      df_win_drop = df_lags.drop('Total Outages', axis=1)\n",
        "\n",
        "      # add relative normalized time points\n",
        "      ptef_array = np.arange(len(df_win_drop))\n",
        "      df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in ptef_array]\n",
        "      df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "      mask = df_win_drop\n",
        "\n",
        "      df_win_no_lags = mask.reset_index(drop=True)\n",
        "      df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "      df_outage_drop_cv = df_outage_drop_cv.reset_index(drop=True)\n",
        "      df_outage_drop_cv_vals = df_outage_drop_cv.values\n",
        "\n",
        "      xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                      df_outage_drop_cv_vals,lag_ext+1)\n",
        "      X_st_s_full.append(xtrain_storms_no_lags)\n",
        "      y_st_s_full.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "    sample_X = np.vstack(X_st_s_full)\n",
        "    sample_y = pd.concat(y_st_s_full)\n",
        "    sample_X = np.array(sample_X)\n",
        "    xtr_reshape = sample_X.reshape(sample_X.shape[0],sample_X.shape[1]*sample_X.shape[-1])\n",
        "\n",
        "    # Normalize regression variables\n",
        "    xtr_scale = X_scaler_full.fit_transform(xtr_reshape)\n",
        "    sample_y_scale = Y_scaler_full.fit_transform(sample_y)\n",
        "    return (xtr_scale,sample_y_scale,X_scaler_full,Y_scaler_full)\n",
        "\n",
        "  else:\n",
        "    X_st_s_df = []\n",
        "    y_st_s_df = []\n",
        "    br = 1\n",
        "    X_scaler_df = MinMaxScaler(feature_range=(0, 1))\n",
        "    Y_scaler_df = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    for c in range(ff[0],ff[1]+1):\n",
        "      df_w_cv = vals_set[c]\n",
        "      df_ws_cv = df_w_cv[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "      df_outage_cv = df_w_cv[['id','Total Outages']]\n",
        "      df_outage_drop_cv = df_outage_cv.drop('id', axis=1)\n",
        "\n",
        "      if df_ws_cv.isnull().values.any():\n",
        "        # Interpolate missing values\n",
        "        df_ws_cv = df_ws_cv.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "        df_ws_cv['id'] = df_ws_cv['id'].astype(int)\n",
        "\n",
        "      df_lags = get_lags(df_ws_cv,lag_ext,raw_trr)\n",
        "      df_win_drop = df_lags.drop('Total Outages', axis=1)\n",
        "\n",
        "      # add relative normalized time points\n",
        "      ptef_array = np.arange(len(df_win_drop))\n",
        "      df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in ptef_array]\n",
        "      df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "      df_win_no_lags = df_win_drop.reset_index(drop=True)\n",
        "      df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "      df_outage_drop_cv = df_outage_drop_cv.reset_index(drop=True)\n",
        "      df_outage_drop_cv_vals = df_outage_drop_cv.values\n",
        "\n",
        "      xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                      df_outage_drop_cv_vals,lag_ext+1)\n",
        "      X_st_s_df.append(xtrain_storms_no_lags)\n",
        "      y_st_s_df.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "    sample_X = np.vstack(X_st_s_df)\n",
        "    sample_y = pd.concat(y_st_s_df)\n",
        "    sample_X = np.array(sample_X)\n",
        "    xtr_reshape = sample_X.reshape(sample_X.shape[0],sample_X.shape[1]*sample_X.shape[-1])\n",
        "\n",
        "    # Normalize regression variables\n",
        "    xtr_scale = X_scaler_df.fit_transform(xtr_reshape)\n",
        "    sample_y_scale = Y_scaler_df.fit_transform(sample_y)\n",
        "    return (xtr_scale,sample_y_scale,X_scaler_df,Y_scaler_df)\n",
        "\n",
        "#@title compute_samples_x_y_test_multiple\n",
        "def compute_samples_x_y_test_multiple(vals_set,ff,lag_ext,raw_trr):\n",
        "  # retrieve train data intervals of windows\n",
        "  X_st_s_test = []\n",
        "  y_st_s_test = []\n",
        "\n",
        "  for c in range(ff[0],ff[1]+1):\n",
        "    df_w_cv_test = vals_set[c]\n",
        "    df_ws_cv_test = df_w_cv_test[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "    df_outage_cv_test = df_w_cv_test[['id','Total Outages']]\n",
        "    df_outage_drop_cv_test = df_outage_cv_test.drop('id', axis=1)\n",
        "\n",
        "    if df_ws_cv_test.isnull().values.any():\n",
        "      # Interpolate missing values\n",
        "      df_ws_cv_test = df_ws_cv_test.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "      df_ws_cv_test['id'] = df_ws_cv_test['id'].astype(int)\n",
        "\n",
        "    df_lags_test = get_lags(df_ws_cv_test,lag_ext,raw_trr)\n",
        "    df_win_drop = df_lags_test.drop('Total Outages', axis=1)\n",
        "    ##df_win_drop = df_ws_lags_test.drop('id', axis=1)\n",
        "\n",
        "    # add relative normalized time points\n",
        "    ptesf_array = np.arange(len(df_win_drop))\n",
        "    df_win_drop['norm_val'] = [(x+1)/len(df_win_drop) for x in ptesf_array]\n",
        "    df_win_drop = df_win_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "    df_win_no_lags = df_win_drop.reset_index(drop=True)\n",
        "    df_win_no_lags_vals = df_win_no_lags.values\n",
        "\n",
        "    df_outage_drop_cv_test = df_outage_drop_cv_test.reset_index(drop=True)\n",
        "    df_outage_drop_cv_test_vals = df_outage_drop_cv_test.values\n",
        "\n",
        "    xtrain_storms_no_lags , ytrain_storms_no_lags = split_sequences_cv(df_win_no_lags_vals,\n",
        "                                                                    df_outage_drop_cv_test_vals,lag_ext+1)\n",
        "    X_st_s_test.append(xtrain_storms_no_lags)\n",
        "    y_st_s_test.append(pd.DataFrame(ytrain_storms_no_lags))\n",
        "\n",
        "  sample_X = np.vstack(X_st_s_test)\n",
        "  sample_y = pd.concat(y_st_s_test)\n",
        "  sample_X = np.array(sample_X)\n",
        "  xtr_reshape = sample_X.reshape(sample_X.shape[0],sample_X.shape[1]*sample_X.shape[-1])\n",
        "  return (xtr_reshape,sample_y,y_st_s_test)\n",
        "\n",
        "#@title k-folds\n",
        "def k_folds(data,split_f):\n",
        "  df_set = data[:]\n",
        "  n_storms = len(df_set)\n",
        "  splits = split_f\n",
        "  n_folds = int(n_storms/splits)\n",
        "\n",
        "  folds = []\n",
        "  for i in range(splits):\n",
        "      if i == 0:\n",
        "        folds.append([i*(n_folds),(i+1)*(n_folds-1)])\n",
        "      else:\n",
        "        folds.append([i*(n_folds),((i+1)*(n_folds))-1])\n",
        "  return folds\n",
        "\n",
        "#@title Avg_CV\n",
        "def Average_cv(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "\n",
        "#@title outages - weather\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from pandas import DataFrame\n",
        "from scipy.stats import boxcox\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "def outs_weas(g_data,g_high_out,r_val):\n",
        "    # parameters\n",
        "    min_ones = 1 # minimum amount of overlapped outliers (ones) between weather and customer outage outliers to call a match\n",
        "    min_consecutive_outage_ones = 0 # minimum number of consecutive ones required to call a storm window\n",
        "\n",
        "    raw_test_w = r_val.values\n",
        "    rtest = DataFrame(raw_test_w)\n",
        "    rtest.columns = ['id','Temp','Preci','Gust','Wind','Windir','LAI','Total Outages']\n",
        "    rtest['id'] = rtest['id'].astype(int)\n",
        "        rtest = rtest.drop('id', axis = 1).reset_index(drop=True)\n",
        "    rtest['id'] = rtest.index\n",
        "    rtest.columns = ['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages','id']\n",
        "    rtest = rtest[['id','Temp','Preci','Gust','Wind','Windir','LAI','Total Outages']]\n",
        "    out_vals = rtest['Total Outages']\n",
        "    wea_ifors_labs  = g_data['anomaly']\n",
        "    out_probs_labs =  g_high_out['Storm_class']\n",
        "\n",
        "    # parameters\n",
        "    min_ones = 1 # minimum amount of overlapped outliers (ones) between weather and customer outage outliers to call a match\n",
        "    min_consecutive_outage_ones = 2 # minimum number of consecutive ones required to call a storm window\n",
        "    w = 5 # window size for calling outliers in the original data\n",
        "\n",
        "    # compute consecutive ones\n",
        "    def zero_runs(outlier_vector, threshold=2):\n",
        "        # Create an array that is 1 where outlier_vector is 0, and pad each end with an extra 0.\n",
        "        iszero = np.concatenate(([0], np.equal(outlier_vector, 1).view(np.int8), [0]))\n",
        "        absdiff = np.abs(np.diff(iszero))\n",
        "        # Runs start and end where absdiff is 1.\n",
        "        ranges = np.where(absdiff == 1)[0].reshape(-1, 2)\n",
        "        filtered_ranges = []\n",
        "        for r in ranges:\n",
        "            if r[-1]-r[0]>=threshold:\n",
        "                filtered_ranges.append(r)\n",
        "        return np.asarray(filtered_ranges)\n",
        "\n",
        "    # retrieve outlier values from storm intervals\n",
        "    b_wea = wea_ifors_labs\n",
        "    # retrieve and compute ranges of consecutive ones for outage intervals\n",
        "    b_outage = out_probs_labs\n",
        "    count_ones_outage = zero_runs(b_outage,min_consecutive_outage_ones)\n",
        "\n",
        "    # customer outage and storm intervals matching algorithm\n",
        "    # declare and initialize variables\n",
        "    out_matches_s = [] # list that holds matched outage intervals indexes\n",
        "    wea_matches_s = [] # list that holds matched weather outliers indexes\n",
        "    visited = [0] # list that holds current indexes of matched weather outliers\n",
        "\n",
        "    previously_used = np.zeros(len(out_probs_labs), dtype = np.int8) # previously visited position\n",
        "    end_wea = len(b_wea)-1 # takes the length of the weather outliers staring from index 0\n",
        "    end_out = len(count_ones_outage)-1 # takes the length of the outage intervals staring from index 0\n",
        "    max_off_set = 24 # the max_hours of weather outliers before outage is reported\n",
        "\n",
        "    \"\"\"\n",
        "    starting from the end of each data - outage intervals and storm outliers  -\n",
        "    for each range of consecutive outage intervals, take max matches of at most 24hrs of past storm intervals\n",
        "    \"\"\"\n",
        "    for ones in range(end_out,0,-1): # start from the end of the outage intervals, pick the last range of consective 1's\n",
        "      # retrive indexes of the last outage range of consective 1's\n",
        "      out_interval = count_ones_outage[ones]\n",
        "\n",
        "      # while we haven't hit the off_set hrs on the weather outliers\n",
        "      off_set = 0\n",
        "      best_match = None # list that holds all offsets and matches in each iteration of outage intervals range\n",
        "      max_val=0\n",
        "      while off_set <= 23:\n",
        "        # compute the number of matches in the two intervals\n",
        "        num_matches=np.sum(b_wea[out_interval[0]-off_set:out_interval[1]-off_set])\n",
        "        # check if this interval overlaps with a previous interval\n",
        "        overlapped_with_previous=np.sum(previously_used[out_interval[0]-off_set:out_interval[1]-off_set])\n",
        "        # store index position of matches, and matches in the matches list\n",
        "        if(num_matches>max_val and overlapped_with_previous==0):\n",
        "            best_match = (off_set, out_interval[0] - off_set, out_interval[1] - off_set, num_matches)\n",
        "            max_val=num_matches\n",
        "        # increment offset value or move to the next offset\n",
        "        off_set = off_set + 1\n",
        "\n",
        "      if best_match is not None and best_match[-1]>=min_ones:\n",
        "          previously_used[best_match[1]:best_match[2]]=1\n",
        "          # append indexes of max weather outliers matches to the weather matches list\n",
        "          wea_matches_s.append((best_match[1], best_match[2]-1))\n",
        "          # store the actual values indexes to the outage matches list\n",
        "          out_matches_s.append((out_interval[0], out_interval[1] - 1))\n",
        "\n",
        "    win_size = 5\n",
        "\n",
        "    # Outage\n",
        "    outage_s = []\n",
        "    for outs in range(len(out_matches_s)):\n",
        "        o_rtr = rtest.loc[(rtest['id']>=out_matches_s[outs][0]) & (rtest['id']<=out_matches_s[outs][-1]+win_size)]\n",
        "        outage_s.append(o_rtr['Total Outages'])\n",
        "\n",
        "    # weather\n",
        "    weather_s = []\n",
        "    for raws in range(len(wea_matches_s)):\n",
        "        w_rtr = rtest.loc[(rtest['id']>=wea_matches_s[raws][0]) & (rtest['id']<=wea_matches_s[raws][-1]+win_size)]\n",
        "        w_rtr.drop('Total Outages',axis=1,inplace=True)\n",
        "        weather_s.append(w_rtr)\n",
        "\n",
        "    # concatenate all data\n",
        "    data_concats_s = []\n",
        "    for i in range(len(weather_s)):\n",
        "        if i >= len(weather_s)-1:\n",
        "            break\n",
        "        we = DataFrame(weather_s[i]).reset_index(drop=True)\n",
        "        ou = DataFrame(outage_s[i]).reset_index(drop=True)\n",
        "        datas = pd.concat([we,ou],axis=1)\n",
        "        data_concats_s.append(datas)\n",
        "    return  data_concats_s, rtest\n",
        "\n",
        "#@title modeling training proportion\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import patsy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def modeling_tr(all_dt_trp,las):\n",
        "\n",
        "  X_st_s_vb_train = []\n",
        "  y_st_s_vb_train = []\n",
        "\n",
        "  train_len = len(all_dt_trp)\n",
        "  vb_train = list([0,train_len-1])\n",
        "\n",
        "  for vb_tr in range(vb_train[0],vb_train[1]+1):\n",
        "    df_w = all_dt_trp[vb_tr]\n",
        "    df_ws = df_w[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "    df_outage = df_w[['id','Total Outages']]\n",
        "    df_outage_drop = df_outage.drop('id', axis=1)\n",
        "\n",
        "    if df_ws.isnull().values.any():\n",
        "      # Interpolate missing values\n",
        "      df_ws = df_ws.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "      df_ws['id'] = df_ws['id'].astype(int)\n",
        "\n",
        "    ##df_lags = get_lags(df_ws,lag)\n",
        "    ##df_win_drop_vb_train = df_lags.drop('Total Outages', axis=1)\n",
        "\n",
        "    df_win_drop_vb_train = df_ws.drop('id', axis=1)\n",
        "\n",
        "    # add relative normalized time points\n",
        "    p_f_array = np.arange(len(df_win_drop_vb_train))\n",
        "    df_win_drop_vb_train['norm_val'] = [(x+1)/len(df_win_drop_vb_train) for x in p_f_array]\n",
        "    df_win_drop_vb_train = df_win_drop_vb_train[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "    df_win_no_lags_vb_train = df_win_drop_vb_train.reset_index(drop=True)\n",
        "    df_win_no_lags_vals_vb_train = df_win_no_lags_vb_train.values\n",
        "    df_outage_drop = df_outage_drop.reset_index(drop=True)\n",
        "    df_outage_drop_vals = df_outage_drop.values\n",
        "\n",
        "    xtrain_storms_no_lags_vb_train , ytrain_storms_no_lags_vb_train = split_sequences_cv(df_win_no_lags_vals_vb_train,\n",
        "                                                                                      df_outage_drop_vals,las+1)\n",
        "    X_st_s_vb_train.append(xtrain_storms_no_lags_vb_train)\n",
        "    y_st_s_vb_train.append(pd.DataFrame(ytrain_storms_no_lags_vb_train))\n",
        "\n",
        "  sample_X_vb_train = np.vstack(X_st_s_vb_train)\n",
        "  sample_y_b_train = pd.concat(y_st_s_vb_train)\n",
        "  sample_X_vb_train = np.array(sample_X_vb_train)\n",
        "  xtr_reshape_vb_train = sample_X_vb_train.reshape(sample_X_vb_train.shape[0],sample_X_vb_train.shape[1]*sample_X_vb_train.shape[-1])\n",
        "\n",
        "  # Apply fit transforms on train data\n",
        "  X_scaler_vb_train = MinMaxScaler(feature_range=(0, 1))\n",
        "  xtr_scale_vb_train  = X_scaler_vb_train.fit_transform(xtr_reshape_vb_train)\n",
        "  Y_scaler_vb_train = MinMaxScaler(feature_range=(0, 1))\n",
        "  ytr_scale_vb_train = Y_scaler_vb_train.fit_transform(sample_y_b_train)\n",
        "\n",
        "  d_x = DataFrame(xtr_scale_vb_train)\n",
        "  d_y = DataFrame(ytr_scale_vb_train)\n",
        "  data_fuse = pd.concat([d_x,d_y],axis=1)\n",
        "  f = 'd_y~d_x - 1'\n",
        "  y , X = patsy.dmatrices(f, data_fuse, return_type='matrix')\n",
        "  ##rf_model = RandomForestRegressor(random_state=0).fit(X,y)\n",
        "  rf_model = neighbors.KNeighborsRegressor().fit(X,y)\n",
        "  return rf_model,X_scaler_vb_train,Y_scaler_vb_train\n",
        "\n",
        "#@title modeling_lags training proportion\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import patsy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def modeling_tr_lags(all_dt_trp,las,rwt):\n",
        "\n",
        "  X_st_s_vb_train = []\n",
        "  y_st_s_vb_train = []\n",
        "\n",
        "  train_len = len(all_dt_trp)\n",
        "  vb_train = list([0,train_len-1])\n",
        "\n",
        "  for vb_tr in range(vb_train[0],vb_train[1]+1):\n",
        "    df_w = all_dt_trp[vb_tr]\n",
        "    df_ws = df_w[['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "    df_outage = df_w[['id','Total Outages']]\n",
        "    df_outage_drop = df_outage.drop('id', axis=1)\n",
        "\n",
        "    if df_ws.isnull().values.any():\n",
        "      # Interpolate missing values\n",
        "      df_ws = df_ws.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0)\n",
        "      df_ws['id'] = df_ws['id'].astype(int)\n",
        "\n",
        "    df_lags = get_lags_test(df_ws,las,rwt)\n",
        "    df_win_drop_vb_train = df_lags.drop('Total Outages', axis=1)\n",
        "\n",
        "    ##df_win_drop_vb_train = df_ws.drop('id', axis=1)\n",
        "\n",
        "    # add relative normalized time points\n",
        "    p_f_array = np.arange(len(df_win_drop_vb_train))\n",
        "    df_win_drop_vb_train['norm_val'] = [(x+1)/len(df_win_drop_vb_train) for x in p_f_array]\n",
        "    df_win_drop_vb_train = df_win_drop_vb_train[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "    df_win_no_lags_vb_train = df_win_drop_vb_train.reset_index(drop=True)\n",
        "    df_win_no_lags_vals_vb_train = df_win_no_lags_vb_train.values\n",
        "    df_outage_drop = df_outage_drop.reset_index(drop=True)\n",
        "    df_outage_drop_vals = df_outage_drop.values\n",
        "\n",
        "    xtrain_storms_no_lags_vb_train , ytrain_storms_no_lags_vb_train = split_sequences_cv(df_win_no_lags_vals_vb_train,\n",
        "                                                                                      df_outage_drop_vals,las+1)\n",
        "    if not xtrain_storms_no_lags_vb_train:\n",
        "      None\n",
        "    else:\n",
        "      X_st_s_vb_train.append(xtrain_storms_no_lags_vb_train)\n",
        "      y_st_s_vb_train.append(pd.DataFrame(ytrain_storms_no_lags_vb_train))\n",
        "\n",
        "  sample_X_vb_train = np.vstack(X_st_s_vb_train)\n",
        "  sample_y_b_train = pd.concat(y_st_s_vb_train)\n",
        "  sample_X_vb_train = np.array(sample_X_vb_train)\n",
        "  xtr_reshape_vb_train = sample_X_vb_train.reshape(sample_X_vb_train.shape[0],sample_X_vb_train.shape[1]*sample_X_vb_train.shape[-1])\n",
        "\n",
        "  # Apply fit transforms on train data\n",
        "  X_scaler_vb_train = MinMaxScaler(feature_range=(0, 1))\n",
        "  xtr_scale_vb_train  = X_scaler_vb_train.fit_transform(xtr_reshape_vb_train)\n",
        "  Y_scaler_vb_train = MinMaxScaler(feature_range=(0, 1))\n",
        "  ytr_scale_vb_train = Y_scaler_vb_train.fit_transform(sample_y_b_train)\n",
        "\n",
        "  d_x = DataFrame(xtr_scale_vb_train)\n",
        "  d_y = DataFrame(ytr_scale_vb_train)\n",
        "  data_fuse = pd.concat([d_x,d_y],axis=1)\n",
        "  f = 'd_y~d_x - 1'\n",
        "  y , X = patsy.dmatrices(f, data_fuse, return_type='matrix')\n",
        "  ##rf_model = RandomForestRegressor(random_state=0).fit(X,y)\n",
        "  rf_model = neighbors.KNeighborsRegressor().fit(X,y)\n",
        "  return rf_model,X_scaler_vb_train,Y_scaler_vb_train\n",
        "\n",
        "#@title split sequences: cv\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences_cv(sequences, y_samples, n_steps):\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequences)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the dataset\n",
        "    if end_ix > len(sequences):\n",
        "      break\n",
        "    # gather input and output parts of the pattern\n",
        "    ##seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "    seq_x, seq_y = sequences[i:end_ix, :], y_samples[i]\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return X, y\n",
        "\n",
        "\n",
        "#@title split a multivariate sequence: test data samples\n",
        "\n",
        "def split_sequences_test(sequences,y_data,n_steps,off):\n",
        "  X, y = list(), list()\n",
        "  for i in range(len(sequences)):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "    # check if we are beyond the dataset\n",
        "    if end_ix + off > len(sequences):\n",
        "    ##if end_ix > len(sequences):\n",
        "      break\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = sequences[i:end_ix, :], y_data[i + off]\n",
        "    ##seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
        "    X.append(seq_x)\n",
        "    y.append(seq_y)\n",
        "  return X, y\n",
        "\n",
        "#@title regression samples _ vals\n",
        "import patsy\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import statsmodels.formula.api as smf\n",
        "from math import sqrt\n",
        "\n",
        "#@title regression samples _ vals\n",
        "import patsy\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import statsmodels.formula.api as smf\n",
        "from math import sqrt\n",
        "\n",
        "def mod_vals(model,X_scaler_tr,Y_scaler_tr,data_c,lg):\n",
        "  X_tests = []\n",
        "  y_tests = []\n",
        "\n",
        "  for full_tes in range(len(data_c)):\n",
        "    df_ws_test_data = data_c[full_tes][['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "    df_outage_test_data_d = data_c[full_tes][['Total Outages']]\n",
        "\n",
        "    df_last_drop = df_ws_test_data.drop('id', axis=1)\n",
        "\n",
        "    if df_last_drop.empty:\n",
        "      None\n",
        "    else:\n",
        "      # add relative normalized time points\n",
        "      p_f_array = np.arange(len(df_last_drop))\n",
        "      df_last_drop['norm_val'] = [(x+1)/len(df_last_drop) for x in p_f_array]\n",
        "      df_last_drop = df_last_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "      df_last_drop = df_last_drop.reset_index(drop=True)\n",
        "      df_last_drop_vals = df_last_drop.values\n",
        "\n",
        "      df_outage_test_data_d = df_outage_test_data_d.reset_index(drop=True)\n",
        "      df_outage_test_data_d_vals = df_outage_test_data_d.values\n",
        "\n",
        "      xtest , ytest = split_sequences_cv(df_last_drop_vals,df_outage_test_data_d_vals,lg+1)\n",
        "\n",
        "      if not xtest:\n",
        "        None\n",
        "      else:\n",
        "        X_tests.append(xtest)\n",
        "        y_tests.append(pd.DataFrame(ytest))\n",
        "\n",
        "  sample_X_test = np.vstack(X_tests)\n",
        "  sample_y_test = pd.concat(y_tests)\n",
        "  sample_y_test = np.array(sample_y_test)\n",
        "  sample_X_test_reshape = sample_X_test.reshape(sample_X_test.shape[0],sample_X_test.shape[1]*sample_X_test.shape[-1])\n",
        "\n",
        "  #@title hourly outages: 2\n",
        "  # Apply transforms on test data: hourly outages\n",
        "  xtr_scale_test = X_scaler_tr.transform(sample_X_test_reshape)\n",
        "\n",
        "  d_x_test = DataFrame(xtr_scale_test)\n",
        "  d_y_test = DataFrame(sample_y_test).reset_index(drop=True)\n",
        "  data_fuse_test = pd.concat([d_x_test,d_y_test],axis=1)\n",
        "  f_test = 'd_y_test~d_x_test - 1'\n",
        "  yst , Xst = patsy.dmatrices(f_test, data_fuse_test, return_type='matrix')\n",
        "  X_test_apply = Xst\n",
        "  y_test_apply = yst.astype(int)\n",
        "\n",
        "  # make prediction on the transformed test data\n",
        "  yhat = model.predict(X_test_apply)\n",
        "  actual_counts = y_test_apply\n",
        "  # Inverse response variable transform\n",
        "  yhat_val = np.array(yhat)\n",
        "  yhat_reshape = yhat_val.reshape(yhat_val.shape[0],1)\n",
        "  predicted_counts = Y_scaler_tr.inverse_transform(yhat_reshape)\n",
        "\n",
        "  # Convert to series\n",
        "  actual_counts = np.array(actual_counts)\n",
        "  ac = actual_counts.ravel()\n",
        "  predicted_counts = np.array(predicted_counts)\n",
        "  pc = predicted_counts.ravel()\n",
        "\n",
        "  #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "  ac_df = DataFrame(ac)\n",
        "  pc_df = DataFrame(pc)\n",
        "  outputs = pd.concat([ac_df,pc_df], axis = 1)\n",
        "  outputs.columns = ['true','fitted']\n",
        "\n",
        "  # Form regression model specification with no intercept\n",
        "  ols_expr = \"\"\"true ~ fitted - 1\"\"\"\n",
        "  # fit the OLS regression model\n",
        "  aux_olsr_results = smf.ols(ols_expr, outputs).fit()\n",
        "  # store R-squared in rmses list\n",
        "  ##print('Test R-squared: %.3f' % (aux_olsr_results.rsquared))\n",
        "\n",
        "  # Compute RMSE\n",
        "  ac_df_rmse = DataFrame(ac)\n",
        "  pc_df_rmse = DataFrame(pc)\n",
        "  mse = mean_squared_error(ac_df_rmse, pc_df_rmse)\n",
        "  rmse = sqrt(mse)\n",
        "\n",
        "  #####################################\n",
        "  # Total Outages\n",
        "  #####################################\n",
        "  wind_observ = []\n",
        "\n",
        "  true_sum_totals = []\n",
        "  for n in range(len(y_tests)):\n",
        "    true_sum_totals.append(np.sum(y_tests[n]))\n",
        "  tru_totals = pd.concat(true_sum_totals)\n",
        "  tru_totals = tru_totals.reset_index(drop=True)\n",
        "\n",
        "  pc_preds = DataFrame(pc)\n",
        "  forecast = pc_preds\n",
        "  preds_sum_totals = []\n",
        "  win_index = 0\n",
        "  for nk in range(len(y_tests)):\n",
        "    win_len = len(y_tests[nk])\n",
        "    # sum forecast windows\n",
        "    preds_sum_totals.append(np.sum(forecast[win_index:win_index + win_len]))\n",
        "    wind_observ.append(forecast[win_index:win_index + win_len])\n",
        "    win_index = win_index + win_len\n",
        "  preds_totals = pd.concat(preds_sum_totals)\n",
        "  preds_totals = preds_totals.reset_index(drop=True)\n",
        "\n",
        "  #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "  ac_totals = DataFrame(tru_totals)\n",
        "  pc_totals = DataFrame(preds_totals)\n",
        "  outputs_totals = pd.concat([ac_totals,pc_totals], axis = 1)\n",
        "  outputs_totals.columns = ['true','fitted']\n",
        "\n",
        "  # Form regression model specification with no intercept\n",
        "  ols_expr_totals = \"\"\"true ~ fitted - 1\"\"\"\n",
        "  # fit the OLS regression model\n",
        "  aux_olsr_results_totals = smf.ols(ols_expr_totals, outputs_totals).fit()\n",
        "  # store R-squared in rmses list\n",
        "  ##print('Test R-squared: %.3f' % (aux_olsr_results_totals.rsquared))\n",
        "\n",
        "  # Compute RMSE\n",
        "  ac_totals_rmse = DataFrame(ac_totals)\n",
        "  pc_totals_rmse = DataFrame(pc_totals)\n",
        "  mse_totals = mean_squared_error(ac_totals_rmse, pc_totals_rmse)\n",
        "  rmse_totals = sqrt(mse_totals)\n",
        "  return aux_olsr_results.rsquared,rmse,aux_olsr_results_totals.rsquared,rmse_totals\n",
        "\n",
        "\n",
        "#@title 5-fold cv on the training data: total outages lags\n",
        "def cv_fold_w_out_total_lag(all_data_train_vals,out_train_raw):\n",
        "  # initialize variables\n",
        "  lookback = 11\n",
        "  num_splits = 5\n",
        "  rmses = []\n",
        "  avgs = []\n",
        "  k = 0 # starting in CV\n",
        "\n",
        "  # first iteration : call k_folds() user-defined function\n",
        "  folds_s = k_folds(all_data_train_vals,num_splits)\n",
        "\n",
        "  for lags in range(lookback):\n",
        "    f_test = folds_s[0]\n",
        "    f_trains = list([folds_s[1][0],folds_s[4][1]])\n",
        "    fs_tr = f_trains\n",
        "    fs_tes = f_test\n",
        "    if lags == 0: # no lags or zero lags\n",
        "      for fold_splits in range(num_splits):\n",
        "        # retrieve training and test samples via compute_samples_x_y_zero_train() and compute_samples_x_y_zero_test()\n",
        "        Xtr , ytr, X_scaler_x, Y_scaler_y = compute_samples_x_y_zero_train(all_data_train_vals,lags,fs_tr)\n",
        "        Xtes , ytes, y_wind = compute_samples_x_y_zero_test(all_data_train_vals,lags,fs_tes)\n",
        "\n",
        "        ##print(fs_tes, fs_tr)\n",
        "        Xtr = DataFrame(Xtr)\n",
        "        ytr = DataFrame(ytr)\n",
        "        Xtes = DataFrame(Xtes)\n",
        "        ytes = DataFrame(ytes)\n",
        "\n",
        "        if Xtr.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtr = np.array(Xtr.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytr.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytr = np.array(ytr.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if Xtes.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtes = np.array(Xtes.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytes.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytes = np.array(ytes.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "\n",
        "        X_train = np.array(Xtr)\n",
        "        y_train = np.array(ytr).ravel()\n",
        "        X_test = Xtes\n",
        "        y_test = ytes\n",
        "\n",
        "        # modeling\n",
        "        knn = neighbors.KNeighborsRegressor()\n",
        "        regressor = knn.fit(X_train,y_train)\n",
        "\n",
        "        # Prediction on validation data\n",
        "        # Apply transforms to regression variables\n",
        "        X_test_scale = X_scaler_x.transform(X_test)\n",
        "        y_test_scale = y_test\n",
        "        y_test_scale = np.array(y_test_scale)\n",
        "        y_test_scale = y_test_scale.ravel()\n",
        "        yhat =  regressor.predict(X_test_scale)\n",
        "        actual_counts = y_test_scale\n",
        "\n",
        "        # Inverse response variable transform\n",
        "        yhat_val = np.array(yhat)\n",
        "        yhat_reshape = yhat_val.reshape(yhat_val.shape[0],1)\n",
        "        predicted_counts = Y_scaler_y.inverse_transform(yhat_reshape)\n",
        "        # Convert to series\n",
        "        ac = actual_counts.ravel()\n",
        "        pc = predicted_counts.ravel()\n",
        "\n",
        "\n",
        "        #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "        ac_df = DataFrame(ac)\n",
        "        pc_df = DataFrame(pc)\n",
        "\n",
        "        true_sum_totals_cv = []\n",
        "        for n in range(len(y_wind)):\n",
        "          true_sum_totals_cv.append(np.sum(y_wind[n]))\n",
        "        tru_totals_cv = pd.concat(true_sum_totals_cv)\n",
        "        tru_totals_cv = tru_totals_cv.reset_index(drop=True)\n",
        "\n",
        "        pc_preds = DataFrame(pc)\n",
        "        forecast = pc_preds\n",
        "        preds_sum_totals_cv = []\n",
        "        win_index = 0\n",
        "        for nk in range(len(y_wind)):\n",
        "          win_len = len(y_wind[nk])\n",
        "          preds_sum_totals_cv.append(np.sum(forecast[win_index:win_index + win_len]))\n",
        "          win_index = win_index + win_len\n",
        "        preds_totals_cv = pd.concat(preds_sum_totals_cv)\n",
        "        preds_totals_cv = preds_totals_cv.reset_index(drop=True)\n",
        "\n",
        "        #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "        ac_totals = DataFrame(tru_totals_cv)\n",
        "        pc_totals = DataFrame(preds_totals_cv)\n",
        "        outputs_totals = pd.concat([ac_totals,pc_totals], axis = 1)\n",
        "        outputs_totals.columns = ['true','fitted']\n",
        "\n",
        "        # Form regression model specification with no intercept\n",
        "        ols_expr_totals = \"\"\"true ~ fitted - 1\"\"\"\n",
        "        # fit the OLS regression model\n",
        "        aux_olsr_results_totals = smf.ols(ols_expr_totals, outputs_totals).fit()\n",
        "        # store R-squared in rmses list\n",
        "        rmses.append(aux_olsr_results_totals.rsquared)\n",
        "\n",
        "        # Prepare next CV fold\n",
        "        if k < 3:\n",
        "          fs_tes = folds_s[k+1]\n",
        "          inter_tes_prev = folds_s[k]\n",
        "          inter_tr = list([folds_s[k+2][0],folds_s[-1][1]])\n",
        "          fs_tr = list([inter_tes_prev,inter_tr])\n",
        "        else:\n",
        "          fs_tes = folds_s[-1]\n",
        "          fs_tr = list([folds_s[0][0],folds_s[3][1]])\n",
        "        k = k + 1\n",
        "\n",
        "      k = 0\n",
        "      average = Average_cv(rmses)\n",
        "      avgs.append([lags,average])\n",
        "      rmses = []\n",
        "      true_sum_totals_cv = []\n",
        "      preds_sum_totals_cv = []\n",
        "\n",
        "    else:  # Where lag is greater than or equals to 1\n",
        "      for fold_splits in range(num_splits):\n",
        "        # retrieve training and test samples via compute_samples_x_y_zero_train() and compute_samples_x_y_zero_test()\n",
        "        Xtr_full , ytr_full, X_scaler_x_full, Y_scaler_y_full = compute_samples_x_y_train_multiple(all_data_train_vals,fs_tr,lags,\n",
        "                                                                                                   out_train_raw)\n",
        "        Xtes_full , ytes_full, y_wind = compute_samples_x_y_test_multiple(all_data_train_vals,fs_tes,lags,out_train_raw)\n",
        "\n",
        "        ##print(fs_tes, fs_tr)\n",
        "        Xtr_full = DataFrame(Xtr_full)\n",
        "        ytr_full = DataFrame(ytr_full)\n",
        "        Xtes_full = DataFrame(Xtes_full)\n",
        "        ytes_full = DataFrame(ytes_full)\n",
        "\n",
        "        if Xtr_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtr_full = np.array(Xtr_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytr_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytr_full = np.array(ytr_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "\n",
        "        if Xtes_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtes_full = np.array(Xtes_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytes_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytes_full = np.array(ytes_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "\n",
        "        X_train = np.array(Xtr_full)\n",
        "        y_train = np.array(ytr_full).ravel()\n",
        "        X_test = Xtes_full\n",
        "        y_test = ytes_full\n",
        "\n",
        "        # modeling\n",
        "        knn = neighbors.KNeighborsRegressor()\n",
        "        regressor = knn.fit(X_train,y_train)\n",
        "\n",
        "        # Prediction on validation data\n",
        "        # Apply transforms to regression variables\n",
        "        X_test_scale = X_scaler_x_full.transform(X_test)\n",
        "        y_test_scale = y_test\n",
        "        y_test_scale = np.array(y_test_scale)\n",
        "        y_test_scale = y_test_scale.ravel()\n",
        "        yhat =  regressor.predict(X_test_scale)\n",
        "        actual_counts = y_test_scale\n",
        "\n",
        "        # Inverse response variable transform\n",
        "        yhat_val = np.array(yhat)\n",
        "        yhat_reshape = yhat_val.reshape(yhat_val.shape[0],1)\n",
        "        predicted_counts = Y_scaler_y_full.inverse_transform(yhat_reshape)\n",
        "        # Convert to series\n",
        "        ac = actual_counts.ravel()\n",
        "        pc = predicted_counts.ravel()\n",
        "\n",
        "        #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "        ac_df = DataFrame(ac)\n",
        "        pc_df = DataFrame(pc)\n",
        "\n",
        "        true_sum_totals_cv = []\n",
        "        for n in range(len(y_wind)):\n",
        "          true_sum_totals_cv.append(np.sum(y_wind[n]))\n",
        "        tru_totals_cv = pd.concat(true_sum_totals_cv)\n",
        "        tru_totals_cv = tru_totals_cv.reset_index(drop=True)\n",
        "\n",
        "        pc_preds = DataFrame(pc)\n",
        "        forecast = pc_preds\n",
        "        preds_sum_totals_cv = []\n",
        "        win_index = 0\n",
        "        for nk in range(len(y_wind)):\n",
        "          win_len = len(y_wind[nk])\n",
        "          preds_sum_totals_cv.append(np.sum(forecast[win_index:win_index + win_len]))\n",
        "          win_index = win_index + win_len\n",
        "        preds_totals_cv = pd.concat(preds_sum_totals_cv)\n",
        "        preds_totals_cv = preds_totals_cv.reset_index(drop=True)\n",
        "\n",
        "        #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "        ac_totals = DataFrame(tru_totals_cv)\n",
        "        pc_totals = DataFrame(preds_totals_cv)\n",
        "        outputs_totals = pd.concat([ac_totals,pc_totals], axis = 1)\n",
        "        outputs_totals.columns = ['true','fitted']\n",
        "\n",
        "        # Form regression model specification with no intercept\n",
        "        ols_expr_totals = \"\"\"true ~ fitted - 1\"\"\"\n",
        "        # fit the OLS regression model\n",
        "        aux_olsr_results_totals = smf.ols(ols_expr_totals, outputs_totals).fit()\n",
        "        # store R-squared in rmses list\n",
        "        rmses.append(aux_olsr_results_totals.rsquared)\n",
        "\n",
        "        # Prepare next CV fold\n",
        "        if k < 3:\n",
        "          fs_tes = folds_s[k+1]\n",
        "          inter_tes_prev = folds_s[k]\n",
        "          inter_tr = list([folds_s[k+2][0],folds_s[-1][1]])\n",
        "          fs_tr = list([inter_tes_prev,inter_tr])\n",
        "        else:\n",
        "          fs_tes = folds_s[-1]\n",
        "          fs_tr = list([folds_s[0][0],folds_s[3][1]])\n",
        "        k = k + 1\n",
        "\n",
        "      k = 0\n",
        "      average = Average_cv(rmses)\n",
        "      avgs.append([lags,average])\n",
        "      rmses = []\n",
        "      true_sum_totals_cv = []\n",
        "      preds_sum_totals_cv = []\n",
        "\n",
        "  idx, max_value= max(avgs, key=lambda item: item[1])\n",
        "\n",
        "  return idx,max_value\n",
        "\n",
        "\n",
        "#@title model selection\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import statsmodels.api as sm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import boxcox\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import neighbors\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import boxcox\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "import patsy\n",
        "import pickle\n",
        "import statsmodels.formula.api as smf\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from scipy import stats\n",
        "\n",
        "def cv_fold_w_out_lag(all_data_train_vals,out_train_raw):\n",
        "  # initialize variables\n",
        "  lookback = 11\n",
        "  num_splits = 5\n",
        "  rmses = []\n",
        "  avgs = []\n",
        "  k = 0 # starting in CV\n",
        "\n",
        "  # first iteration : call k_folds() user-defined function\n",
        "  folds_s = k_folds(all_data_train_vals,num_splits)\n",
        "\n",
        "  for lags in range(lookback):\n",
        "    f_test = folds_s[0]\n",
        "    f_trains = list([folds_s[1][0],folds_s[4][1]])\n",
        "    fs_tr = f_trains\n",
        "    fs_tes = f_test\n",
        "    if lags == 0: # no lags or zero lags\n",
        "      for fold_splits in range(num_splits):\n",
        "        # retrieve training and test samples via compute_samples_x_y_zero_train() and compute_samples_x_y_zero_test()\n",
        "        Xtr , ytr, X_scaler_x, Y_scaler_y = compute_samples_x_y_zero_train(all_data_train_vals,lags,fs_tr)\n",
        "        Xtes , ytes, yw = compute_samples_x_y_zero_test(all_data_train_vals,lags,fs_tes)\n",
        "\n",
        "        ##print(fs_tes, fs_tr)\n",
        "        Xtr = DataFrame(Xtr)\n",
        "        ytr = DataFrame(ytr)\n",
        "        Xtes = DataFrame(Xtes)\n",
        "        ytes = DataFrame(ytes)\n",
        "\n",
        "        if Xtr.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtr = np.array(Xtr.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytr.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytr = np.array(ytr.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if Xtes.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtes = np.array(Xtes.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytes.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytes = np.array(ytes.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "\n",
        "        X_train = np.array(Xtr)\n",
        "        y_train = np.array(ytr).ravel()\n",
        "        X_test = Xtes\n",
        "        y_test = ytes\n",
        "\n",
        "        # modeling\n",
        "        knn = neighbors.KNeighborsRegressor()\n",
        "        regressor = knn.fit(X_train,y_train)\n",
        "\n",
        "        # Prediction on validation data\n",
        "        # Apply transforms to regression variables\n",
        "        X_test_scale = X_scaler_x.transform(X_test)\n",
        "        ##X_test_scale  = pd.DataFrame(stats.zscore(X_test))\n",
        "        y_test_scale = y_test\n",
        "        y_test_scale = np.array(y_test_scale)\n",
        "        y_test_scale = y_test_scale.ravel()\n",
        "        yhat =  regressor.predict(X_test_scale)\n",
        "        actual_counts = y_test_scale\n",
        "\n",
        "        # Inverse response variable transform\n",
        "        yhat_val = np.array(yhat)\n",
        "        yhat_reshape = yhat_val.reshape(yhat_val.shape[0],1)\n",
        "        predicted_counts = Y_scaler_y.inverse_transform(yhat_reshape)\n",
        "        # Convert to series\n",
        "        ac = actual_counts.ravel()\n",
        "        pc = predicted_counts.ravel()\n",
        "\n",
        "        #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "        ac_df = DataFrame(ac)\n",
        "        pc_df = DataFrame(pc)\n",
        "        outputs = pd.concat([ac_df,pc_df], axis = 1)\n",
        "        outputs.columns = ['true','fitted']\n",
        "\n",
        "        # Form regression model specification with no intercept\n",
        "        ols_expr = \"\"\"true ~ fitted - 1\"\"\"\n",
        "        # fit the OLS regression model\n",
        "        aux_olsr_results = smf.ols(ols_expr, outputs).fit()\n",
        "        # store R-squared in rmses list\n",
        "        rmses.append(aux_olsr_results.rsquared)\n",
        "\n",
        "        # Prepare next CV fold\n",
        "        if k < 3:\n",
        "          fs_tes = folds_s[k+1]\n",
        "          inter_tes_prev = folds_s[k]\n",
        "          inter_tr = list([folds_s[k+2][0],folds_s[-1][1]])\n",
        "          fs_tr = list([inter_tes_prev,inter_tr])\n",
        "        else:\n",
        "          fs_tes = folds_s[-1]\n",
        "          fs_tr = list([folds_s[0][0],folds_s[3][1]])\n",
        "        k = k + 1\n",
        "\n",
        "      k = 0\n",
        "\n",
        "      average = Average_cv(rmses)\n",
        "      avgs.append([lags,average])\n",
        "      rmses = []\n",
        "\n",
        "    else:  # Where lag is greater than or equals to 1\n",
        "      for fold_splits in range(num_splits):\n",
        "        # retrieve training and test samples via compute_samples_x_y_zero_train() and compute_samples_x_y_zero_test()\n",
        "        Xtr_full , ytr_full, X_scaler_x_full,Y_scaler_y_full = compute_samples_x_y_train_multiple(all_data_train_vals,fs_tr,lags,\n",
        "                                                                                                  out_train_raw)\n",
        "        Xtes_full , ytes_full, yw_full = compute_samples_x_y_test_multiple(all_data_train_vals,fs_tes,lags,out_train_raw)\n",
        "\n",
        "        ##print(fs_tes, fs_tr)\n",
        "        Xtr_full = DataFrame(Xtr_full)\n",
        "        ytr_full = DataFrame(ytr_full)\n",
        "        Xtes_full = DataFrame(Xtes_full)\n",
        "        ytes_full = DataFrame(ytes_full)\n",
        "\n",
        "        if Xtr_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtr_full = np.array(Xtr_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytr_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytr_full = np.array(ytr_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "\n",
        "        if Xtes_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          Xtes_full = np.array(Xtes_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "        if ytes_full.isnull().values.any():\n",
        "          # Interpolate missing values\n",
        "          ytes_full = np.array(ytes_full.interpolate(method ='linear', limit_direction ='forward', limit = 100, axis=0))\n",
        "\n",
        "        X_train = np.array(Xtr_full)\n",
        "        y_train = np.array(ytr_full).ravel()\n",
        "        X_test = Xtes_full\n",
        "        y_test = ytes_full\n",
        "\n",
        "        # modeling\n",
        "        knn = neighbors.KNeighborsRegressor()\n",
        "        regressor = knn.fit(X_train,y_train)\n",
        "\n",
        "        # Prediction on validation data\n",
        "        # Apply transforms to regression variables\n",
        "        X_test_scale = X_scaler_x_full.transform(X_test)\n",
        "        y_test_scale = y_test\n",
        "        y_test_scale = np.array(y_test_scale)\n",
        "        y_test_scale = y_test_scale.ravel()\n",
        "        yhat =  regressor.predict(X_test_scale)\n",
        "        actual_counts = y_test_scale\n",
        "\n",
        "        # Inverse response variable transform\n",
        "        yhat_val = np.array(yhat)\n",
        "        yhat_reshape = yhat_val.reshape(yhat_val.shape[0],1)\n",
        "        predicted_counts = Y_scaler_y_full.inverse_transform(yhat_reshape)\n",
        "\n",
        "        # Convert to series\n",
        "        ac = actual_counts.ravel()\n",
        "        pc = predicted_counts.ravel()\n",
        "\n",
        "        #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "        ac_df = DataFrame(ac)\n",
        "        pc_df = DataFrame(pc)\n",
        "        outputs = pd.concat([ac_df,pc_df], axis = 1)\n",
        "        outputs.columns = ['true','fitted']\n",
        "\n",
        "        # Form regression model specification with no intercept\n",
        "        ols_expr = \"\"\"true ~ fitted - 1\"\"\"\n",
        "        # fit the OLS regression model\n",
        "        aux_olsr_results = smf.ols(ols_expr, outputs).fit()\n",
        "        # store R-squared in rmses list\n",
        "        rmses.append(aux_olsr_results.rsquared)\n",
        "\n",
        "        # Prepare next CV fold\n",
        "        if k < 3:\n",
        "          fs_tes = folds_s[k+1]\n",
        "          inter_tes_prev = folds_s[k]\n",
        "          inter_tr = list([folds_s[k+2][0],folds_s[-1][1]])\n",
        "          fs_tr = list([inter_tes_prev,inter_tr])\n",
        "        else:\n",
        "          fs_tes = folds_s[-1]\n",
        "          fs_tr = list([folds_s[0][0],folds_s[3][1]])\n",
        "        k = k + 1\n",
        "\n",
        "      k = 0\n",
        "      average = Average_cv(rmses)\n",
        "      ##avgs.append((average))\n",
        "      avgs.append([lags,average])\n",
        "      rmses = []\n",
        "  idx, max_value= max(avgs, key=lambda item: item[1])\n",
        "  return idx, max_value\n",
        "\n",
        "#@title get_lags_train\n",
        "def get_lags(extend_samples_tr,la_tr,raw_train):\n",
        "  lags_full_data = []\n",
        "  ty_lag_tr = extend_samples_tr.iloc[[0]]['id']\n",
        "  ty_lag_tr = np.array(ty_lag_tr)\n",
        "  ty_lag_tr = ty_lag_tr.item()\n",
        "\n",
        "  ty_lag_las = extend_samples_tr.iloc[[-1]]['id']\n",
        "  ty_lag_tr_las = np.array(ty_lag_las)\n",
        "  ty_lag_tr_las = ty_lag_tr_las.item()\n",
        "\n",
        "  start_lag_tr = ty_lag_tr\n",
        "  end_lag_tr = abs(start_lag_tr - la_tr)\n",
        "  lags_df  = raw_train.values\n",
        "\n",
        "  for val_lags in range(end_lag_tr,ty_lag_tr_las+1):\n",
        "    values = lags_df[val_lags]\n",
        "    values_df = DataFrame(values)\n",
        "    lags_full_data.append(values_df.T)\n",
        "\n",
        "  all_data_slags = []\n",
        "  for z in range(len(lags_full_data)):\n",
        "    ds_lags = pd.DataFrame(lags_full_data[z])\n",
        "    slags_df = pd.concat([ds_lags])\n",
        "    slags_df.columns = ['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages']\n",
        "    all_data_slags.append(slags_df)\n",
        "  slagFull_set = pd.concat(all_data_slags)\n",
        "  return pd.concat([slagFull_set]).reset_index(drop=True)\n",
        "\n",
        "#@title regression samples _ vals_lags\n",
        "import patsy\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import statsmodels.formula.api as smf\n",
        "from math import sqrt\n",
        "\n",
        "def mod_vals_lags(model,X_scaler_tr,Y_scaler_tr,data_c,lg,rwtest):\n",
        "  X_tests = []\n",
        "  y_tests = []\n",
        "\n",
        "  for full_tes in range(len(data_c)):\n",
        "    df_ws_test_data = data_c[full_tes][['id','Temp','Preci','Gust','Wind','Windir','LAI']]\n",
        "    df_outage_test_data_d = data_c[full_tes][['Total Outages']]\n",
        "\n",
        "    df_lags_test_data = get_lags_test(df_ws_test_data,lg,rwtest)\n",
        "    df_lags_test_data_idw = df_lags_test_data.drop('id', axis=1)\n",
        "    df_lags_test_data_tot = df_lags_test_data_idw.drop('Total Outages', axis=1)\n",
        "    df_last_drop = df_lags_test_data_tot\n",
        "\n",
        "    if df_last_drop.empty:\n",
        "      None\n",
        "    else:\n",
        "      # add relative normalized time points\n",
        "      p_f_array = np.arange(len(df_last_drop))\n",
        "      df_last_drop['norm_val'] = [(x+1)/len(df_last_drop) for x in p_f_array]\n",
        "      df_last_drop = df_last_drop[['Temp','Preci','Gust','Wind','Windir','LAI','norm_val']]\n",
        "\n",
        "      df_last_drop = df_last_drop.reset_index(drop=True)\n",
        "      df_last_drop_vals = df_last_drop.values\n",
        "\n",
        "      df_outage_test_data_d = df_outage_test_data_d.reset_index(drop=True)\n",
        "      df_outage_test_data_d_vals = df_outage_test_data_d.values\n",
        "\n",
        "      xtest , ytest = split_sequences_cv(df_last_drop_vals,df_outage_test_data_d_vals,lg+1)\n",
        "\n",
        "      if not xtest:\n",
        "        None\n",
        "      else:\n",
        "        X_tests.append(xtest)\n",
        "        y_tests.append(pd.DataFrame(ytest))\n",
        "\n",
        "  sample_X_test = np.vstack(X_tests)\n",
        "  sample_y_test = pd.concat(y_tests)\n",
        "  sample_y_test = np.array(sample_y_test)\n",
        "  sample_X_test_reshape = sample_X_test.reshape(sample_X_test.shape[0],sample_X_test.shape[1]*sample_X_test.shape[-1])\n",
        "\n",
        "  #@title hourly outages: 2\n",
        "  # Apply transforms on test data: hourly outages\n",
        "  xtr_scale_test = X_scaler_tr.transform(sample_X_test_reshape)\n",
        "\n",
        "  d_x_test = DataFrame(xtr_scale_test)\n",
        "  d_y_test = DataFrame(sample_y_test).reset_index(drop=True)\n",
        "  data_fuse_test = pd.concat([d_x_test,d_y_test],axis=1)\n",
        "  f_test = 'd_y_test~d_x_test - 1'\n",
        "  yst , Xst = patsy.dmatrices(f_test, data_fuse_test, return_type='matrix')\n",
        "  X_test_apply = Xst\n",
        "  y_test_apply = yst.astype(int)\n",
        "\n",
        "  # make prediction on the transformed test data\n",
        "  yhat = model.predict(X_test_apply)\n",
        "  actual_counts = y_test_apply\n",
        "  # Inverse response variable transform\n",
        "  yhat_val = np.array(yhat)\n",
        "  yhat_reshape = yhat_val.reshape(yhat_val.shape[0],1)\n",
        "  predicted_counts = Y_scaler_tr.inverse_transform(yhat_reshape)\n",
        "\n",
        "  # Convert to series\n",
        "  actual_counts = np.array(actual_counts)\n",
        "  ac = actual_counts.ravel()\n",
        "  predicted_counts = np.array(predicted_counts)\n",
        "  pc = predicted_counts.ravel()\n",
        "\n",
        "  #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "  ac_df = DataFrame(ac)\n",
        "  pc_df = DataFrame(pc)\n",
        "  outputs = pd.concat([ac_df,pc_df], axis = 1)\n",
        "  outputs.columns = ['true','fitted']\n",
        "  ##outputs.to_csv('file.csv')\n",
        "\n",
        "  # Form regression model specification with no intercept\n",
        "  ols_expr = \"\"\"true ~ fitted - 1\"\"\"\n",
        "  # fit the OLS regression model\n",
        "  aux_olsr_results = smf.ols(ols_expr, outputs).fit()\n",
        "\n",
        "  # Compute RMSE\n",
        "  ac_df_rmse = DataFrame(ac)\n",
        "  pc_df_rmse = DataFrame(pc)\n",
        "  mse = mean_squared_error(ac_df_rmse, pc_df_rmse)\n",
        "  rmse = sqrt(mse)\n",
        "\n",
        "  #####################################\n",
        "  # Total Outages\n",
        "  #####################################\n",
        "  wind_observ = []\n",
        "\n",
        "  true_sum_totals = []\n",
        "  for n in range(len(y_tests)):\n",
        "    true_sum_totals.append(np.sum(y_tests[n]))\n",
        "  tru_totals = pd.concat(true_sum_totals)\n",
        "  tru_totals = tru_totals.reset_index(drop=True)\n",
        "\n",
        "  pc_preds = DataFrame(pc)\n",
        "  forecast = pc_preds\n",
        "  preds_sum_totals = []\n",
        "  win_index = 0\n",
        "  for nk in range(len(y_tests)):\n",
        "    win_len = len(y_tests[nk])\n",
        "    # sum forecast windows\n",
        "    preds_sum_totals.append(np.sum(forecast[win_index:win_index + win_len]))\n",
        "    wind_observ.append(forecast[win_index:win_index + win_len])\n",
        "    win_index = win_index + win_len\n",
        "  preds_totals = pd.concat(preds_sum_totals)\n",
        "  preds_totals = preds_totals.reset_index(drop=True)\n",
        "\n",
        "  #OLS regression model with no intercept : just a straight line passing through the origin\n",
        "  ac_totals = DataFrame(tru_totals)\n",
        "  pc_totals = DataFrame(preds_totals)\n",
        "  outputs_totals = pd.concat([ac_totals,pc_totals], axis = 1)\n",
        "  outputs_totals.columns = ['true','fitted']\n",
        "  # Save output file\n",
        "  ##outputs_totals.to_csv('file.csv')\n",
        "\n",
        "  # Form regression model specification with no intercept\n",
        "  ols_expr_totals = \"\"\"true ~ fitted - 1\"\"\"\n",
        "  # fit the OLS regression model\n",
        "  aux_olsr_results_totals = smf.ols(ols_expr_totals, outputs_totals).fit()\n",
        "  # store R-squared in rmses list\n",
        "  ##print('Test R-squared: %.3f' % (aux_olsr_results_totals.rsquared))\n",
        "\n",
        "  # Compute RMSE\n",
        "  ac_totals_rmse = DataFrame(ac_totals)\n",
        "  pc_totals_rmse = DataFrame(pc_totals)\n",
        "  mse_totals = mean_squared_error(ac_totals_rmse, pc_totals_rmse)\n",
        "  rmse_totals = sqrt(mse_totals)\n",
        "\n",
        "  return aux_olsr_results.rsquared,rmse,aux_olsr_results_totals.rsquared,rmse_totals\n",
        "\n",
        "\n",
        "#@title get_lags_train\n",
        "def get_lags(extend_samples_tr,la_tr,raw_train):\n",
        "  lags_full_data = []\n",
        "  ty_lag_tr = extend_samples_tr.iloc[[0]]['id']\n",
        "  ty_lag_tr = np.array(ty_lag_tr)\n",
        "  ty_lag_tr = ty_lag_tr.item()\n",
        "\n",
        "  ty_lag_las = extend_samples_tr.iloc[[-1]]['id']\n",
        "  ty_lag_tr_las = np.array(ty_lag_las)\n",
        "  ty_lag_tr_las = ty_lag_tr_las.item()\n",
        "\n",
        "  start_lag_tr = ty_lag_tr\n",
        "  end_lag_tr = abs(start_lag_tr - la_tr)\n",
        "  lags_df  = raw_train.values\n",
        "\n",
        "  for val_lags in range(end_lag_tr,ty_lag_tr_las+1):\n",
        "    values = lags_df[val_lags]\n",
        "    values_df = DataFrame(values)\n",
        "    lags_full_data.append(values_df.T)\n",
        "\n",
        "  all_data_slags = []\n",
        "  for z in range(len(lags_full_data)):\n",
        "    ds_lags = pd.DataFrame(lags_full_data[z])\n",
        "    slags_df = pd.concat([ds_lags])\n",
        "    slags_df.columns = ['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages']\n",
        "    all_data_slags.append(slags_df)\n",
        "  slagFull_set = pd.concat(all_data_slags)\n",
        "  return pd.concat([slagFull_set]).reset_index(drop=True)\n",
        "\n",
        "#@title get lags test data\n",
        "def get_lags_test(d_ws,la,df_lag):\n",
        "  ty_lag = d_ws.iloc[[0]]['id']\n",
        "  ty_lag = np.array(ty_lag)\n",
        "  ty_lag = ty_lag.item()\n",
        "\n",
        "  ty_lag_e = d_ws.iloc[[-1]]['id']\n",
        "  ty_lag_e = np.array(ty_lag_e)\n",
        "  ty_lag_e = ty_lag_e.item()\n",
        "\n",
        "  start_lag = ty_lag\n",
        "  end_lag = abs(start_lag - la)\n",
        "\n",
        "  orig_data_lag = df_lag.loc[(df_lag['id']>=end_lag) & (df_lag['id']<=ty_lag_e)]\n",
        "  orig_data_lag.columns = ['id','Temp','Preci','Gust','Wind','Windir','LAI','Total Outages']\n",
        "  return orig_data_lag\n",
        "\n",
        "#@title compute weather storm intervals -  gap concepts\n",
        "def wea_outlier_r(outlier_data,g):\n",
        "  # iForest based on matched trained data\n",
        "  outlier_data['id'] = outlier_data.index\n",
        "  outlier_data.columns = ['Temp_min','Temp_max','Preci_max','Preci_avg','Gust_max','Gust_avg',\n",
        "                            'Wind_max','Wind_avg','Windir_avg','Windir_std','LAI_avg','scores','anomaly','id']\n",
        "  # rearrange columns\n",
        "  outlier_data = outlier_data[['id','Temp_min','Temp_max','Preci_max','Preci_avg','Gust_max','Gust_avg',\n",
        "                          'Wind_max','Wind_avg','Windir_avg','Windir_std','LAI_avg','scores','anomaly']]\n",
        "\n",
        "  tes_outlier = outlier_data\n",
        "  # storm windows based on iForest outlier labels:\n",
        "  ids_labs_gabs = []\n",
        "\n",
        "  count = 0\n",
        "  count_r = 0\n",
        "  count_l = 0\n",
        "  n_r = 0\n",
        "  n_l = 0\n",
        "  m = 0\n",
        "  count_t  = 0\n",
        "\n",
        "  for k in range(len(tes_outlier)):\n",
        "    if m >= len(tes_outlier):\n",
        "          break\n",
        "    while not tes_outlier['anomaly'].iloc[m].item() == 0:\n",
        "      ids_labs_gabs.append(tes_outlier['anomaly'].iloc[m].item())\n",
        "      m = m + 1\n",
        "      if m >= len(tes_outlier):\n",
        "          break\n",
        "\n",
        "    if m >= len(tes_outlier):\n",
        "          break\n",
        "\n",
        "    if tes_outlier['anomaly'].iloc[m].item()== 0:\n",
        "      n_r = m\n",
        "      n_l = m\n",
        "      count_r = count_r + 1\n",
        "      count_l = count_l + 1\n",
        "\n",
        "    while tes_outlier['anomaly'].iloc[n_r].item()== 0:\n",
        "      count_r = count_r + 1\n",
        "      n_r = n_r + 1\n",
        "\n",
        "      if n_r >= len(tes_outlier):\n",
        "        break\n",
        "\n",
        "    while tes_outlier['anomaly'].iloc[n_l].item()== 0:\n",
        "      count_l = count_l + 1\n",
        "      n_l = n_l - 1\n",
        "\n",
        "    count = count_r + count_l\n",
        "    if count <= g:\n",
        "      ids_labs_gabs.append(1)\n",
        "    else:\n",
        "      ids_labs_gabs.append(0)\n",
        "\n",
        "    m = m + 1\n",
        "    count = 0\n",
        "    count_r = 0\n",
        "    count_l = 0\n",
        "\n",
        "  labs_storm_df = DataFrame(ids_labs_gabs)\n",
        "  arr_labs = np.array(labs_storm_df)\n",
        "  arr_labs = arr_labs.ravel()\n",
        "  outlier_data['anomaly'] = arr_labs\n",
        "  outlier_data.drop(columns=['scores'], inplace=True)\n",
        "  return  outlier_data\n",
        "\n",
        "# main function\n",
        "#@title gridsearch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "from patsy import dmatrices\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn import model_selection\n",
        "\n",
        "sns.set_style('white')\n",
        "%matplotlib inline\n",
        "\n",
        "##!pip install -U kaleido\n",
        "\n",
        "win = 6\n",
        "overlap = 5\n",
        "rng = np.random.RandomState(42)\n",
        "\n",
        "mydata = pd.read_csv('nas.csv',header=0)\n",
        "dataIn = mydata[mydata.columns[1:8]]\n",
        "dateT = mydata[mydata.columns[0]]\n",
        "dateT = DataFrame(dateT)\n",
        "dateT['dateTime'] = dateT\n",
        "dateT.drop('Unnamed: 0', axis = 1, inplace = True)\n",
        "df_data = pd.concat((dateT, dataIn), axis=1)\n",
        "df_data = df_data.drop(df_data.columns[0], axis = 1)\n",
        "\n",
        "# Drop rows which contain any NaN values in all columns\n",
        "df_data = df_data.dropna( how='all')\n",
        "\n",
        "# Interpolate missing values\n",
        "df_data = df_data.interpolate(method ='linear', limit_direction ='both', limit = 10000, axis=0)\n",
        "\n",
        "# move target variable to the last column to align with surpervise learning struture\n",
        "finData = pd.DataFrame(df_data,columns=['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages'])\n",
        "\n",
        "# Clean dataset\n",
        "finData[finData['Preci']<0]  = 0\n",
        "finData['LAI'][10565] = 0.7\n",
        "# replace with NANs\n",
        "finData['Temp'][1800:2700] = np.nan\n",
        "finData['Preci'][1800:2700] = np.nan\n",
        "finData['Gust'][1800:2700] = np.nan\n",
        "finData['Wind'][1800:2700] = np.nan\n",
        "finData['Windir'][1800:2700] = np.nan\n",
        "\n",
        "# Interpolate missing values\n",
        "w_out_data = finData.interpolate(method ='linear', limit_direction ='forward', limit = 10000, axis=0)\n",
        "nas_wea_per = w_out_data\n",
        "\n",
        "# split into train and test set\n",
        "outlier_train = nas_wea_per\n",
        "out_size = int(len(outlier_train) * .8)\n",
        "df_outlier_tr, df_outlier_tes = outlier_train[:out_size], outlier_train[out_size:]\n",
        "raw_tr_val = df_outlier_tr\n",
        "df_outlier_tr_drop = df_outlier_tr.drop('Total Outages', axis = 1)\n",
        "df_outlier_tes_drop = df_outlier_tes.drop('Total Outages', axis = 1)\n",
        "\n",
        "# Read probabilistic - high outage interval data\n",
        "d_high = pd.read_csv('file',header=0,index_col=0)\n",
        "\n",
        "# Create validation data for conbination of hyperparamenters\n",
        "df_train = df_outlier_tr_drop[:int(df_outlier_tr_drop.shape[0]*0.8)]\n",
        "raw_s_train = df_outlier_tr[:int(df_outlier_tr.shape[0]*0.8)]\n",
        "df_val = df_outlier_tr_drop[len(df_train):]\n",
        "raw_val = df_outlier_tr[len(df_train):]\n",
        "\n",
        "# high outage intervals\n",
        "d_high_out_tr = d_high[:int(d_high.shape[0]*0.8)]\n",
        "d_high_out_val = d_high[len(d_high_out_tr):]\n",
        "\n",
        "raw_s_train['id'] = raw_s_train.index\n",
        "raw_s_train.columns = [['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages','id']]\n",
        "# rearrange columns\n",
        "raw_s_train = raw_s_train[['id','Temp','Preci','Gust','Wind','Windir','LAI','Total Outages']]\n",
        "\n",
        "raw_val['id'] = raw_val.index\n",
        "raw_val.columns = [['Temp','Preci','Gust','Wind','Windir','LAI','Total Outages','id']]\n",
        "# rearrange columns\n",
        "raw_val = raw_val[['id','Temp','Preci','Gust','Wind','Windir','LAI','Total Outages']]\n",
        "\n",
        "# Break training data into windows: 6hrs and 5hrs overlap\n",
        "wea_train_data = mapping_series(df_train,win,overlap)\n",
        "# Break validation data into windows: 6hrs and 5hrs overlap\n",
        "wea_val_data = mapping_series(df_val,win,overlap)\n",
        "\n",
        "# fit iForest model to validation data\n",
        "val_iforest = ifor_vals(wea_val_data)\n",
        "val_iforest.drop(columns=['scores'], inplace=True)\n",
        "\n",
        "# matched outages to weather\n",
        "mat_val_w_out_data,out_data_vals = outs_weas(val_iforest,d_high_out_val,raw_val)\n",
        "\n",
        "# grid list\n",
        "conta = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "gaps = [1, 5, 8, 20, 40]\n",
        "rmses = []\n",
        "r2s = []\n",
        "gaps_lst = []\n",
        "cons = []\n",
        "\n",
        "# fit the model\n",
        "for u in range(len(conta)):\n",
        "    for z in range(len(gaps)):\n",
        "        clf =IsolationForest(max_samples='auto',contamination=float(conta[u]),random_state=rng)\n",
        "        clf.fit(wea_train_data)\n",
        "        wea_pd = pd.DataFrame(wea_train_data)\n",
        "        new_wea_train_data = wea_pd.copy()\n",
        "        new_wea_train_data['scores'] = clf.decision_function(wea_pd)\n",
        "        new_wea_train_data['anomaly'] = clf.predict(wea_pd)\n",
        "        new_wea_train_data['anomaly'] = new_wea_train_data['anomaly'].map({1:0,-1:1})\n",
        "\n",
        "        # compute contiguous weather outlier regions\n",
        "        gapped_data = wea_outlier_r(new_wea_train_data,gaps[z])\n",
        "        mat_tr_w_out_data,out_data_trains = outs_weas(gapped_data,d_high_out_tr,raw_s_train)\n",
        "\n",
        "        # compute best set of lag parameters: CV\n",
        "        drop_id_raw_train = out_data_trains.drop('id',axis=1)\n",
        "        # total outages\n",
        "        lgt,max_t = cv_fold_w_out_total_lag(mat_tr_w_out_data,drop_id_raw_train)\n",
        "\n",
        "        if lgt == 0:\n",
        "          model, x_scal, y_scal = modeling_tr(mat_tr_w_out_data,lgt)\n",
        "          r_sq,rme,r_sq_tot,rme_tot = mod_vals(model,x_scal,y_scal,mat_val_w_out_data,lgt)\n",
        "          print('Contamination:', conta[u], '; Gaps:', gaps[z], '; R_squared:',round(r_sq,3),'; RMSE:',round(rme,3))\n",
        "          r2s.append([r_sq,conta[u],gaps[z],lgt,rme,max_t])\n",
        "\n",
        "        else:\n",
        "          id_raw_train = out_data_trains\n",
        "          ##model, x_scal, y_scal = modeling_tr(mat_tr_w_out_data,lgt)\n",
        "          model, x_scal, y_scal = modeling_tr_lags(mat_tr_w_out_data,lgt,id_raw_train)\n",
        "          id_raw_test = out_data_vals\n",
        "          r_sq,rme,r_sq_tot,rme_tot = mod_vals_lags(model,x_scal,y_scal,mat_val_w_out_data,lgt,id_raw_test)\n",
        "          print('Contamination:', conta[u], '; Gaps', gaps[z], '; R_squared:',round(r_sq,3),'; RMSE:',round(rme,3))\n",
        "          r2s.append([r_sq,conta[u],gaps[z],lgt,rme,max_t])\n",
        "\n",
        "r2_value,ct,gz,la,rme_value,cv_val = max(r2s, key=lambda item: item[0])\n",
        "print('Best:','R_squared:', round(r2_value,3), \"; Contamination:\",ct, \"; Gaps:\",gz, \"; Lag:\",la, '; RMSE:', round(rme_value,3) )\n",
        "# save hyperameters\n",
        "with open('file','wb') as ftes:\n",
        "          pickle.dump(r2s,ftes)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
